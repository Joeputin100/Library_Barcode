{
  "project_overview": {
    "goal": "Create a Streamlit application that takes a CSV of book data, fetches metadata from external APIs (Google Books, Library of Congress), uses Vertex AI for classification, and generates printable labels. The application is deployed on Streamlit Cloud via GitHub.",
    "repository": "https://github.com/Joeputin100/Library_Barcode",
    "primary_file": "streamlit_app.py",
    "deployment_environment": "Streamlit Cloud"
  },
  "problem_summary": {
    "current_blocker": "A persistent `SyntaxError: unterminated f-string literal` on line 361 of `streamlit_app.py` prevents the Streamlit application from running.",
    "error_details": {
      "file": "/mount/src/library_barcode/streamlit_app.py",
      "line_number": 361,
      "error_type": "SyntaxError",
      "message": "unterminated f-string literal",
      "offending_line": "st_logger.debug(f\"LOC query for '{title}' by '{author}': {base_url}?{requests.compat.urlencode(params)})\"
    },
    "symptoms": "The Streamlit application fails to compile and run, displaying the `SyntaxError` in the Streamlit Cloud logs.",
    "history": "Multiple attempts to fix this syntax error by correcting the f-string have been made. Each attempt involved overwriting the `streamlit_app.py` file with the corrected code and pushing the changes to the `master` branch on GitHub. Despite the git push being successful and the code appearing correct locally, the error persists on Streamlit Cloud, suggesting a potential deployment, caching, or platform-specific issue beyond simple code correction."
  },
  "feature_requests_and_secondary_issues": [
    {
      "id": "MLA_CAPITALIZATION",
      "request": "Capitalize the 'Title' field to align with MLA standards.",
      "status": "Implemented",
      "implementation_details": "A function `capitalize_title_mla(title)` was created and applied to the title data before display. This was part of the same commits as the failed syntax error fixes."
    },
    {
      "id": "VERTEX_AI_ATTRIBUTE_ERROR",
      "request": "Fix an `AttributeError` that occurred when processing results from the Vertex AI batch classification.",
      "status": "Attempted Fix Implemented",
      "implementation_details": "The error was traced to calling `.lower()` on a potentially non-string value from the Vertex AI response. The matching logic was updated to be more flexible and to include an `isinstance(item_title, str)` check to prevent the error. This was part of the same commits as the failed syntax error fixes."
    },
    {
      "id": "DATA_INTEGRATION_ISSUES",
      "request": "Resolve data integration problems, specifically with missing series numbers and incorrect classifications for certain books (e.g., 'Jack & Jill', 'Genius Prince's Guide').",
      "status": "Attempted Fix Implemented",
      "implementation_details": "The regular expression for series number extraction was improved, and the matching logic for Vertex AI results was made more flexible to handle title variations. These fixes were part of the same commits as the failed syntax error fixes."
    }
  ],
  "file_content": {
    "streamlit_app_py": "import streamlit as st\nimport pandas as pd\nimport re\nimport requests\nfrom lxml import etree\nimport time\nimport json\nimport os\nimport threading\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nimport xml.etree.ElementTree as ET\nimport logging\nfrom datetime import datetime\nimport io\n\n# --- Logging Setup ---\nlog_capture_string = io.StringIO()\nst_logger = logging.getLogger()\nst_logger.setLevel(logging.DEBUG)\nst_handler = logging.StreamHandler(log_capture_string)\nst_handler.setFormatter(logging.Formatter('%(levelname)s: %(message)s'))\nst_logger.addHandler(st_handler)\nfrom bs4 import BeautifulSoup\nimport vertexai\nimport pytz\nfrom vertexai.generative_models import GenerativeModel\n\n# --- Page Title ---\nst.title(\"Atriuum Label Generator\")\nst.caption(f\"Last updated: {datetime.now(pytz.timezone('US/Pacific')).strftime('%Y-%m-%d %H:%M:%S %Z%z')}\")\n\n# --- Constants & Cache ---\nSUGGESTION_FLAG = \"üêí\"\nCACHE_FILE = \"loc_cache.json\"\n\n# --- Caching Functions ---\ndef load_cache():\n    if os.path.exists(CACHE_FILE):\n        with open(CACHE_FILE, 'r') as f:\n            return json.load(f)\n    return {}\n\ndef save_cache(cache):\n    with open(CACHE_FILE, 'w') as f:\n        json.dump(cache, f, indent=4)\n\ndef clean_title(title):\n    \"\"\"Cleans title by moving leading articles to the end.\"\"\"\n    if not isinstance(title, str):\n        return \"\"\n    articles = ['The ', 'A ', 'An ']\n    for article in articles:\n        if title.startswith(article):\n            return title[len(article):] + \", \" + title[:len(article)-1]\n    return title\n\ndef capitalize_title_mla(title):\n    \"\"\"Capitalizes a title according to MLA standards.\"\"\"\n    if not isinstance(title, str) or not title:\n        return \"\"\n    \n    words = title.lower().split()\n    # List of articles, prepositions, and conjunctions that should not be capitalized unless they are the first or last word.\n    minor_words = ['a', 'an', 'the', 'and', 'but', 'or', 'for', 'nor', 'on', 'at', 'to', 'from', 'by', 'in', 'of', 'off', 'out', 'up', 'so', 'yet']\n    \n    capitalized_words = []\n    for i, word in enumerate(words):\n        if i == 0 or i == len(words) - 1 or word not in minor_words:\n            capitalized_words.append(word.capitalize())\n        else:\n            capitalized_words.append(word)\n            \n    return \" \" .join(capitalized_words)\n\ndef clean_author(author):\n    \"\"\"Cleans author name to Last, First Middle.\"\"\"\n    if not isinstance(author, str):\n        return \"\"\n    parts = author.split(',')\n    if len(parts) == 2:\n        return f\"{parts[0].strip()}, {parts[1].strip()}\"\n    return author\n\n# --- Helper Functions ---\ndef get_book_metadata_google_books(title, author, cache):\n    \"\"\"Fetches book metadata from the Google Books API.\"\"\"\n    safe_title = re.sub(r'[^a-zA-Z0-9\\s\\.:]', '', title)\n    safe_author = re.sub(r'[^a-zA-Z0-9\\s,]', '', author)\n    cache_key = f\"google_{safe_title}|{safe_author}\".lower()\n    if cache_key in cache:\n        st.write(f\"DEBUG: Google Books cache hit for '{title}' by '{author}'.\")\n        return cache[cache_key]\n\n    metadata = {'google_genres': [], 'classification': '', 'series_name': '', 'volume_number': '', 'publication_year': '', 'error': None}\n    try:\n        query = f'intitle:\"{safe_title}\" + inauthor:\"{safe_author}\" ' \n        url = f\"https://www.googleapis.com/books/v1/volumes?q={query}&maxResults=1\"\n        st_logger.debug(f\"Google Books query for '{title}' by '{author}': {url}\")\n        response = requests.get(url, timeout=15)\n        response.raise_for_status()\n        data = response.json()\n        st_logger.debug(f\"Google Books raw response for '{title}' by '{author}': {response.text}\")\n\n        if \"items\" in data and data[\"items\"]:\n            item = data[\"items\"][0]\n            volume_info = item.get(\"volumeInfo\", {}\n)
            st_logger.debug(f\"Google Books volume_info for '{title}': {volume_info}\")\n            st_logger.info(f\"Google Books genres for '{title}': {volume_info.get('categories', [])}\")\n\n            if \"categories\" in volume_info:\n                metadata['google_genres'].extend(volume_info[\"categories\"])\n            \n            if \"description\" in volume_info:\n                description = volume_info[\"description\"]\n                match = re.search(r'Subject: (.*?)(?:\\n|$)', description, re.IGNORECASE)\n                if match:\n                    subjects = [s.strip() for s in match.group(1).split(',') ]\n                    metadata['google_genres'].extend(subjects)\n                    st_logger.info(f\"Google Books subjects for '{title}': {subjects}\")\n\n            if 'publishedDate' in volume_info:\n                metadata['publication_year'] = extract_year(volume_info['publishedDate'])\n\n            if 'seriesInfo' in volume_info:\n                series_info = volume_info['seriesInfo']\n                if 'bookDisplayNumber' in series_info:\n                    metadata['volume_number'] = series_info['bookDisplayNumber']\n                if 'series' in series_info and series_info['series']:\n                    if 'title' in series_info['series'][0]:\n                        metadata['series_name'] = series_info['series'][0]['title']\n\n            # Series number extraction from title\n            if not metadata['volume_number']:\n                volume_match = re.search(r'(volume|vol\\.|book|part|vol|bk\\.|v)\s*(\d+|[ivxlcdm]+|one|two|three|four|five|six|seven|eight|nine|ten)[\\s\\)]*', title, re.IGNORECASE)\n                if volume_match:\n                    volume_str = volume_match.group(2)\n                    if volume_str.isdigit():\n                        metadata['volume_number'] = volume_str\n                    else:\n                        word_to_num = {'one': '1', 'two': '2', 'three': '3', 'four': '4', 'five': '5', 'six': '6', 'seven': '7', 'eight': '8', 'nine': '9', 'ten': '10'}\n                        metadata['volume_number'] = word_to_num.get(volume_str.lower())\n\n            if not metadata['volume_number'] and any(c.lower() in ['manga', 'comic'] for c in metadata['genres']):
                trailing_num_match = re.search(r'(\d+)$', title)\n                if trailing_num_match:\n                    metadata['volume_number'] = trailing_num_match.group(1)\n
        cache[cache_key] = metadata\n        return metadata\n
    except requests.exceptions.RequestException as e:\n        metadata['error'] = f"Google Books API request failed: {e}"\n    except Exception as e:\n        metadata['error'] = f"An unexpected error occurred with Google Books API: {e}"\n    return metadata\n\ndef get_vertex_ai_classification_batch(batch_books, vertex_ai_credentials):\n    \"\"\"Uses a Generative AI model on Vertex AI to classify a batch of books' genres.\"\"\"\n    temp_creds_path = \"temp_creds.json\"\n    retry_delays = [10, 20, 30] # Increased delays for Vertex AI retries\n    \n    try:\n        credentials_dict = dict(vertex_ai_credentials)\n        credentials_json = json.dumps(credentials_dict)\n        \n        with open(temp_creds_path, \"w\") as f:\n            f.write(credentials_json)\n\n        os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = temp_creds_path\n        \n        vertexai.init(project=credentials_dict[\"project_id\"], location=\"us-central1\")\n        model = GenerativeModel(\"gemini-2.5-flash\")\n        \n        # Construct a single prompt for the batch\n        batch_prompts = []\n        for book in batch_books:\n            batch_prompts.append(f\"Title: {book['title']}, Author: {book['author']}\")\n        \n        full_prompt = (\n            \"For each book in the following list, provide its primary genre or Dewey Decimal classification, series title, volume number, and copyright year. \"\n            \"If it's fiction, classify as 'FIC'. If non-fiction, provide a general Dewey Decimal category like '300' for Social Sciences, '500' for Science, etc. \"\n            \"Provide the output as a JSON array of objects, where each object has 'title', 'author', 'classification', 'series_title', 'volume_number', and 'copyright_year' fields. \"\n            \"If you cannot determine a value for a field, use an empty string ''.\\n\\n\" +\n            \"Books:\\n\" + \"\\n\".join(batch_prompts)\n        )\n        st_logger.debug(f\"Vertex AI full prompt:\\n```\\n{full_prompt}\\n```\")\n        \n        for i in range(len(retry_delays) + 1):\n            try:\n                response = model.generate_content(full_prompt)\n                # Attempt to parse JSON response\n                response_text = response.text.strip()\n                st_logger.debug(f\"Vertex AI raw response:\\n```\\n{response_text}\\n```\")\n                # Clean up markdown code block if present\n                if response_text.startswith(\"```json\") and response_text.endswith(\"```\"):\n                    response_text = response_text[7:-3].strip()\n                \n                classifications = json.loads(response_text)\n                \n                # Create a dictionary for easy lookup\n                classified_results = {}\n                for item in classifications:\n                    key = f\"{item['title']}|{item['author']}\".lower()\n                    classified_results[key] = {\n                        'classification': item.get('classification', 'Unknown'),\n                        'series_title': item.get('series_title', ''),\n                        'volume_number': item.get('volume_number', ''),\n                        'copyright_year': item.get('copyright_year', '')\n                    }\n                return classified_results\n            except Exception as e:\n                if i < len(retry_delays):\n                    st_logger.warning(f\"Vertex AI batch call failed: {e}. Retrying in {retry_delays[i]} seconds...\")\n                    time.sleep(retry_delays[i])\n                else:\n                    st_logger.error(f\"Vertex AI batch call failed after multiple retries: {e}\")\n                    return {f\"{book['title']}|{book['author']}\".lower(): {'classification': 'Unknown', 'series_title': '', 'volume_number': '', 'copyright_year': ''} for book in batch_books}\n    finally:\n        if os.path.exists(temp_creds_path):\n            os.remove(temp_creds_path)\n\nLCC_TO_DDC_MAP = {\n    'AC': '080-089', 'AE': '030-039', 'AG': '030-039', 'AI': '050-059', 'AM': '060-069',\n    'AS': '060-069', 'AY': '031-032', 'B': '100-109', 'BC': '160-169', 'BD': '110-119',\n    'BF': '150-159', 'BH': '111, 701', 'BJ': '170-179', 'BL': '200-299', 'BM': '200-299',\n    'BP': '200-299', 'BQ': '200-299', 'BR': '200-299', 'BS': '200-299', 'BT': '200-299',\n    'BV': '200-299', 'BX': '200-299', 'CB': '909', 'CC': '930-939', 'CD': '091, 930',\n    'CE': '529', 'CJ': '737', 'CN': '411, 930', 'CR': '929.6', 'CS': '929.1-929.5',\n    'CT': '920-929', 'D': '909', 'DA': '940-999', 'DB': '940-999', 'DC': '940-999',\n    'DD': '940-999', 'DE': '940-999', 'DF': '940-999', 'DG': '940-999', 'DH': '940-999',\n    'DJ': '940-999', 'DK': '940-999', 'DL': '940-999', 'DP': '940-999', 'DQ': '940-999',\n    'DR': '940-999', 'DS': '940-999', 'DT': '940-999', 'DU': '940-999', 'DX': '940-999',\n    'E': '970-979', 'F': '973-999', 'G': '910-919', 'GB': '910.02', 'GC': '551.46',\n    'GE': '333.7, 577', 'GF': '304.2, 301-309', 'GN': '301-309', 'GR': '398',\n    'GT': '390-399', 'GV': '790-799', 'H': '300-309', 'HA': '310-319', 'HB': '330-339',\n    'HC': '330-339', 'HD': '330-339', 'HE': '380-389', 'HF': '330-339', 'HG': '330-339',\n    'HH': '330-339', 'HJ': '330-339', 'HM': '301-309', 'HN': '301-309', 'HQ': '301-309',\n    'HS': '301-309', 'HT': '301-309', 'HV': '301-309', 'HX': '335', 'J': '320',\n    'JA': '320.01', 'JC': '320.1-320.5', 'JF': '320-329', 'JJ': '320-329', 'JK': '320-329',\n    'JL': '320-329', 'JN': '320-329', 'JQ': '320-329', 'JS': '320-329', 'JV': '320-329',\n    'JX': '341-349', 'K': '340', 'KBM': '340-349', 'KBP': '340-349', 'KBR': '340-349',\n    'KBS': '340-349', 'KBT': '340-349', 'KBU': '340-349', 'L': '370', 'LA': '370-375',\n    'LB': '370-375', 'LC': '371-379', 'LD': '378', 'LE': '378', 'LF': '378', 'LG': '378',\n    'M': '780', 'ML': '780.9', 'MT': '781-789', 'N': '700-709', 'NA': '720-729',\n    'NB': '730-779', 'NC': '730-779', 'ND': '730-779', 'NE': '730-779', 'NK': '730-779',\n    'NX': '730-779', 'P': '400-409', 'PA': '880-889', 'PB': '410-419', 'PC': '440-469',\n    'PD': '430-439', 'PE': '420-429', 'PF': '430-439', 'PG': '891.7', 'PH': '494',\n    'PJ': '892', 'PK': '891', 'PL': '895', 'PM': '497, 499', 'PN': '800-809',\n    'PQ': '840-849', 'PR': '820-829', 'PS': '810-819', 'PT': '830-839', 'PZ': 'FIC',\n    'Q': '500-509', 'QA': '510-519', 'QB': '520-599', 'QC': '520-599', 'QD': '520-599',\n    'QE': '520-599', 'QH': '520-599', 'QK': '520-599', 'QL': '520-599', 'QM': '520-599',\n    'QP': '520-599', 'QR': '520-599', 'R': '610', 'RA': '610-619', 'RB': '610-619',\n    'RC': '610-619', 'RD': '610-619', 'RE': '610-619', 'RF': '610-619', 'RG': '610-619',\n    'RJ': '610-619', 'RK': '610-619', 'RL': '610-619', 'RM': '610-619', 'RS': '610-619',\n    'RT': '610-619', 'RV': '610-619', 'RX': '610-619', 'RZ': '610-619', 'S': '630',\n    'SB': '630-639', 'SD': '630-639', 'SF': '630-639', 'SH': '630-639', 'SK': '630-639',\n    'T': '600-609', 'TA': '620-699', 'TC': '620-699', 'TD': '620-699', 'TE': '620-699',\n    'TF': '620-699', 'TG': '620-699', 'TH': '620-699', 'TJ': '620-699', 'TK': '620-699',\n    'TL': '620-699', 'TN': '620-699', 'TP': '620-699', 'TR': '620-699', 'TS': '620-699',\n    'TT': '620-699', 'TX': '620-699', 'U': '355', 'UA': '355-359', 'UB': '355-359',\n    'UC': '355-359', 'UD': '355-359', 'UE': '355-359', 'UF': '355-359', 'UG': '355-359',\n    'UH': '355-359', 'V': '359', 'VM': '623', 'Z': '010-029'\n}\n\ndef lcc_to_ddc(lcc):\n    \"\"\"Converts an LCC call number to a DDC range or 'FIC'.\"\"\"\n    if not isinstance(lcc, str) or not lcc:\n        return \"\"\n\n    if lcc == \"FIC\":\n        return \"FIC\"\n\n    # Check for fiction-heavy classes first\n    if lcc.startswith(('PZ', 'PQ', 'PR', 'PS', 'PT')):\n        return \"FIC\"\n\n    # General mapping\n    for prefix, ddc_range in LCC_TO_DDC_MAP.items():\n        if lcc.startswith(prefix):\n            return ddc_range.split('-')[0].strip()\n            \n    return \"\" # Return empty if no match found\n\ndef clean_call_number(call_num_str, genres, google_genres=None, title="", is_original_data=False):\n    st_logger.debug(f\"clean_call_number input: call_num_str='{call_num_str}', genres={genres}, google_genres={google_genres}, title='{title}', is_original_data={is_original_data}\")\n    if google_genres is None:\n        google_genres = []\n        \n    if not isinstance(call_num_str, str):\n        st_logger.debug(f\"clean_call_number returning empty string for non-string input: {call_num_str}\")\n        return \"\" # Default for non-string input\n\n    cleaned = call_num_str.strip()\n    # Only remove suggestion flag if it's not original data\n    if not is_original_data:\n        cleaned = cleaned.lstrip(SUGGESTION_FLAG)\n\n    # Prioritize Google Books categories and other genre lists for FIC\n    fiction_keywords_all = [\"fiction\", \"fantasy\", \"science fiction\", \"thriller\", \"mystery\", \"romance\", \"horror\", \"novel\", \"stories\", \"a novel\", \"young adult fiction\", \"historical fiction\", \"literary fiction\"]\n    if any(g.lower() in fiction_keywords_all for g in google_genres) or \
       any(genre.lower() in fiction_keywords_all for genre in genres) or \
       any(keyword in title.lower() for keyword in fiction_keywords_all):\n        st_logger.debug(f\"clean_call_number returning FIC based on genre/title keywords: {cleaned}\")\n        return \"FIC\"\n\n    # Check for LCC\n    ddc_from_lcc = lcc_to_ddc(cleaned)\n    if ddc_from_lcc:\n        st_logger.debug(f\"clean_call_number returning from LCC: {ddc_from_lcc}\")\n        return ddc_from_lcc\n\n    # Strip common non-numeric characters from Dewey-like numbers\n    cleaned = re.sub(r'[^a-zA-Z0-9\\s\\.:]', '', cleaned).strip()\n\n    # If the cleaned string is a known non-numeric genre from Vertex AI, map to FIC\n    # This catches cases where Vertex AI directly returns a genre name\n    if cleaned.lower() in [\"fantasy\", \"science fiction\", \"thriller\", \"mystery\", \"romance\", \"horror\", \"novel\", \"fiction\", \"young adult fiction\", \"historical fiction\", \"literary fiction\"]:\n        st_logger.debug(f\"clean_call_number returning FIC based on cleaned string match: {cleaned}\")\n        return \"FIC\"\n\n    # Check for explicit \"FIC\" or valid Dewey Decimal from any source\n    if cleaned.upper().startswith(\"FIC\"):\n        st_logger.debug(f\"clean_call_number returning FIC based on explicit FIC: {cleaned}\")\n        return \"FIC\"\n    \n    # Strict check for Dewey Decimal Number format (3 digits, optional decimal and more digits)\n    if re.match(r'^\\d{3}(\\.\\d+)?$', cleaned):\n        st_logger.debug(f\"clean_call_number returning Dewey Decimal: {cleaned}\")\n        return cleaned\n    \n    # If it's a number but not 3 digits, still return it (e.g., from LOC 050 field like \"PS3515.E37\"), keep it as is\n    # This allows for LC call numbers to pass through if they are numeric-like\n    if re.match(r'^[A-Z]{1,3}\\d+(\\.\\d+)?$', cleaned) or re.match(r'^\\d+(\\.\\d+)?$', cleaned):\n        st_logger.debug(f\"clean_call_number returning LC-like or numeric: {cleaned}\")\n        return cleaned\n\n    # If none of the above conditions are met, it's an invalid format for a call number\n    st_logger.debug(f\"clean_call_number returning empty string for invalid format: {cleaned}\")\n    return \"\"\n\ndef get_book_metadata_initial_pass(title, author, cache, is_blank=False, is_problematic=False):\n    st_logger.debug(f\"Entering get_book_metadata_initial_pass for: {title}\")\n    safe_title = re.sub(r'[^a-zA-Z0-9\\s\\.:]', '', title)\n    safe_author = re.sub(r'[^a-zA-Z0-9\\s,]', '', author)\n\n    metadata = {'classification': '', 'series_name': '', 'volume_number': '', 'publication_year': '', 'genres': [], 'google_genres': [], 'error': None}\n\n    google_meta = get_book_metadata_google_books(title, author, cache)\n    metadata.update(google_meta)\n\n    loc_cache_key = f\"loc_{safe_title}|{safe_author}\".lower()\n    if loc_cache_key in cache and 'series_name' in cache[loc_cache_key] and 'publication_year' in cache[loc_cache_key]:\n        st_logger.debug(f\"LOC cache hit for '{title}' by '{author}'.\")\n        cached_loc_meta = cache[loc_cache_key]\n        metadata.update(cached_loc_meta)\n    else:\n        base_url = \"http://lx2.loc.gov:210/LCDB\"\n        query = f'bath.title=\"{safe_title}\" and bath.author=\"{safe_author}\" ' \n        params = {\"version\": \"1.1\", \"operation\": \"searchRetrieve\", \"query\": query, \"maximumRecords\": \"1\", \"recordSchema\": \"marcxml\"}\n        st_logger.debug(f\"LOC query for '{title}' by '{author}': {base_url}?{requests.compat.urlencode(params)}\") # Fixed syntax error\n        \n        retry_delays = [5, 15, 30]\n        for i in range(len(retry_delays) + 1):\n            try:\n                response = requests.get(base_url, params=params, timeout=20)\n                response.raise_for_status()\n                st_logger.debug(f\"LOC raw response for '{title}' by '{author}':\\n```xml\\n{response.content.decode('utf-8')}\\n```\")\n                root = etree.fromstring(response.content)\n                ns_diag = {'diag': 'http://www.loc.gov/zing/srw/diagnostic/'}\n                error_message = root.find('.//diag:message', ns_diag)\n                if error_message is not None:\n                    metadata['error'] = f\"LOC API Error: {error_message.text}\"\n                else:\n                    ns_marc = {'marc': 'http://www.loc.gov/MARC21/slim'}\n                    classification_node = root.find('.//marc:datafield[@tag="082"]/marc:subfield[@code="a"]', ns_marc)\n                    if classification_node is not None: metadata['classification'] = classification_node.text.strip()\n                    series_node = root.find('.//marc:datafield[@tag="490"]/marc:subfield[@code="a"]', ns_marc)\n                    if series_node is not None: metadata['series_name'] = series_node.text.strip().rstrip(' ;')\n                    volume_node = root.find('.//marc:datafield[@tag="490"]/marc:subfield[@code="v"]', ns_marc)\n                    if volume_node is not None: metadata['volume_number'] = volume_node.text.strip()\n                    pub_year_node = root.find('.//marc:datafield[@tag="264"]/marc:subfield[@code="c"]', ns_marc) or root.find('.//marc:datafield[@tag="260"]/marc:subfield[@code="c"]', ns_marc)\n                    if pub_year_node is not None and pub_year_node.text:\n                        years = re.findall(r'(1[7-9]\d{2}|20\d{2})', pub_year_node.text)\n                        if years: metadata['publication_year'] = str(min([int(y) for y in years]))\n                    genre_nodes = root.findall('.//marc:datafield[@tag="655"]/marc:subfield[@code="a"]', ns_marc)\n                    if genre_nodes:\n                        metadata['genres'] = [g.text.strip().rstrip('.') for g in genre_nodes]\n                        st_logger.info(f\"LOC genres for '{title}': {metadata['genres']}\")\n                    \n                    # Series number extraction from title\n                    if not metadata['volume_number']:\n                        volume_match = re.search(r'(volume|vol\\.|book|part|vol|bk\\.|v)\s*(\d+|[ivxlcdm]+|one|two|three|four|five|six|seven|eight|nine|ten)[\\s\\)]*', title, re.IGNORECASE)\n                        if volume_match:\n                            volume_str = volume_match.group(2)\n                            if volume_str.isdigit():\n                                metadata['volume_number'] = volume_str\n                            else:\n                                word_to_num = {'one': '1', 'two': '2', 'three': '3', 'four': '4', 'five': '5', 'six': '6', 'seven': '7', 'eight': '8', 'nine': '9', 'ten': '10'}\n                                metadata['volume_number'] = word_to_num.get(volume_str.lower())\n\n                    if not metadata['volume_number'] and any(c.lower() in ['manga', 'comic'] for c in metadata['genres']):\n                        trailing_num_match = re.search(r'(\d+)$', title)\n                        if trailing_num_match:\n                            metadata['volume_number'] = trailing_num_match.group(1)\n\n                    # Only cache successful LOC lookups\n                    if not metadata['error']:\n                        cache[loc_cache_key] = metadata\n                break # Exit retry loop on success\n            except requests.exceptions.RequestException as e:\n                if i < len(retry_delays):\n                    st_logger.warning(f\"LOC API call failed for {title}. Retrying in {retry_delays[i]}s...\")\n                    time.sleep(retry_delays[i])\n                    continue\n                metadata['error'] = f\"LOC API request failed after retries: {e}\"\n                st_logger.error(f\"LOC failed for {title}, returning what we have.\")\n            except Exception as e:\n                metadata['error'] = f\"An unexpected error occurred with LOC API: {e}\"\n                st_logger.error(f\"Unexpected LOC error for {title}, returning what we have.\")\n                break\n\n    return metadata\n\ndef generate_pdf_labels(df):\n    buffer = io.StringIO()\n    c = canvas.Canvas(buffer, pagesize=letter)\n    width, height = letter\n\n    # Label dimensions (example, adjust as needed)\n    label_width = 2.5 * inch\n    label_height = 1 * inch\n    x_margin = 0.5 * inch\n    y_margin = 0.5 * inch\n    x_spacing = 0.25 * inch\n    y_spacing = 0.25 * inch\n\n    labels_per_row = int((width - 2 * x_margin + x_spacing) / (label_width + x_spacing))\n    labels_per_col = int((height - 2 * y_margin + y_spacing) / (label_height + y_spacing))\n\n    for i, row in df.iterrows():\n        col_num = i % labels_per_row\n        row_num = (i // labels_per_row) % labels_per_col\n        \n        if i > 0 and col_num == 0 and row_num == 0:\n            c.showPage()\n\n        x = x_margin + col_num * (label_width + x_spacing)\n        y = height - y_margin - (row_num + 1) * (label_height + y_spacing)\n\n        # Draw border for label (for debugging layout)\n        # c.rect(x, y, label_width, label_height)\n\n        # Extract and clean data for label\n        holding_barcode = str(row['Holdings Barcode']).replace(SUGGESTION_FLAG, '')\n        call_number = str(row['Call Number']).replace(SUGGESTION_FLAG, '')\n        title = str(row['Title'])\n        author = str(row['Author'])\n        series_info = str(row['Series Info']).replace(SUGGESTION_FLAG, '')\n        copyright_year = str(row['Copyright Year']).replace(SUGGESTION_FLAG, '')\n\n        # Set font and size\n        c.setFont('Helvetica', 10)\n\n        # Draw Holding Number\n        c.drawString(x + 0.1 * inch, y + label_height - 0.2 * inch, holding_barcode)\n\n        # Draw Call Number\n        c.drawString(x + 0.1 * inch, y + label_height - 0.35 * inch, call_number)\n\n        # Draw Title (truncate if too long)\n        max_title_width = label_width - 0.2 * inch\n        if c.stringWidth(title, 'Helvetica', 10) > max_title_width:\n            while c.stringWidth(title + '...', 'Helvetica', 10) > max_title_width:\n                title = title[:-1]\n            title += '...'
        c.drawString(x + 0.1 * inch, y + label_height - 0.5 * inch, title)\n\n        # Draw Author\n        c.drawString(x + 0.1 * inch, y + label_height - 0.65 * inch, author)\n\n        # Draw Series Info and Copyright Year\n        bottom_text = []\n        if series_info: bottom_text.append(series_info)\n        if copyright_year: bottom_text.append(copyright_year)\n        \n        if bottom_text:\n            c.drawString(x + 0.1 * inch, y + label_height - 0.8 * inch, ', '.join(bottom_text))\n\n    c.save()\n    return buffer.getvalue()\n\ndef extract_year(date_string):\n    \"\"\"Extracts the first 4-digit number from a string, assuming it's a year.\"\"\"\n    if isinstance(date_string, str):\n        # Regex to find a 4-digit year, ignoring surrounding brackets, c, or ¬©\n        match = re.search(r'[\\(\\)\[¬©c]?(d{4})[\\)\]]?', date_string)\n        if match:\n            return match.group(1)\n    return \"\"\n\ndef main():\n    st_logger.debug(\"Streamlit app main function started.\")\n    uploaded_file = st.file_uploader(\"Upload your Atriuum CSV Export\", type=\"csv\")\n\n    if uploaded_file:\n        df = pd.read_csv(uploaded_file, encoding='latin1', dtype=str).fillna('')\n        loc_cache = load_cache()\n        \n        st.write(\"Processing rows...\")\n        progress_bar = st.progress(0)\n\n        # Get Vertex AI credentials once on the main thread\n        vertex_ai_credentials = None\n        try:\n            vertex_ai_credentials = st.secrets[\"vertex_ai\"]\n        except Exception as e:\n            st.error(f\"Error loading Vertex AI credentials from st.secrets: {e}\")\n            st.info(\"Please ensure you have configured your Vertex AI credentials in Streamlit secrets. See README for instructions.\")\n            return # Stop execution if credentials are not available\n\n        results = [{} for _ in range(len(df))]\n        unclassified_books_for_vertex_ai = [] # To collect books for batch Vertex AI processing\n\n        # First pass: Process with Google Books and LOC APIs\n        with ThreadPoolExecutor(max_workers=5) as executor:\n            futures = {}\n            for i, row in df.iterrows():\n                title = row.get('Title', '').strip()\n                author = row.get(\"Author's Name\", '').strip()\n                is_blank_row = not title and not author\n                \n                problematic_books = [\n                    (\"The Genius Prince's Guide to Raising a Nation Out of Debt (Hey, How About Treason?), Vol. 5\", \"Toba, Toru\"),\n                    (\"The old man and the sea\", \"Hemingway, Ernest\"),\n                    (\"Jack & Jill (Alex Cross)\", \"Patterson, James\"),\n                ]\n                is_problematic_row = (title, author) in problematic_books\n\n                future = executor.submit(get_book_metadata_initial_pass, title, author, loc_cache, is_blank=is_blank_row, is_problematic=is_problematic_row)\n                futures[future] = i\n\n            for i, future in enumerate(as_completed(futures)):\n                row_index = futures[future]\n                lc_meta = future.result()\n                st_logger.debug(f\"lc_meta for row {row_index}: {lc_meta}\")\n                row = df.iloc[row_index]\n                title = row.get('Title', '').strip()\n                author = row.get(\"Author's Name\", '').strip()\n\n                problematic_books = [\n                    (\"The Genius Prince's Guide to Raising a Nation Out of Debt (Hey, How About Treason?), Vol. 5\", \"Toba, Toru\"),\n                    (\"The old man and the sea\", \"Hemingway, Ernest\"),\n                    (\"Jack & Jill (Alex Cross)\", \"Patterson, James\"),\n                ]\n                is_problematic = (title, author) in problematic_books\n\n                if is_problematic:\n                    st_logger.info(f\"--- Problematic Book Detected: {title} by {author} ---\")\n                \n                # Original Atriuum data\n                original_holding_barcode = row.get('Holdings Barcode', '').strip()\n                raw_original_call_number = row.get('Call Number', '').strip() # Get raw original\n                cleaned_original_call_number = clean_call_number(raw_original_call_number, [], [], title=\"", is_original_data=True) # Clean original\n                original_series_name = row.get('Series Title', '').strip()\n                original_series_number = row.get('Series Number', '').strip()\n                original_copyright_year = extract_year(row.get('Copyright', '').strip())\n                original_publication_date_year = extract_year(row.get('Publication Date', '').strip())\n\n                # Prioritize original copyright/publication year\n                final_original_year = \"\"\n                if original_copyright_year: final_original_year = original_copyright_year\n                elif original_publication_date_year: final_original_year = original_publication_date_year\n\n                # Mashed-up data from initial pass\n                api_call_number = lc_meta.get('classification', '')\n                cleaned_call_number = clean_call_number(api_call_number, lc_meta.get('genres', []), lc_meta.get('google_genres', []), title=title)\n                mashed_series_name = lc_meta.get('series_name', '').strip()\n                mashed_volume_number = lc_meta.get('volume_number', '').strip()\n                mashed_publication_year = lc_meta.get('publication_year', '').strip()\n\n                # Merge logic for initial pass\n                # Use cleaned_original_call_number if valid, else fallback to mashed-up\n                current_call_number = cleaned_original_call_number if cleaned_original_call_number else (SUGGESTION_FLAG + cleaned_call_number if cleaned_call_number else \"\")\n                current_series_name = original_series_name if original_series_name else (SUGGESTION_FLAG + mashed_series_name if mashed_series_name else '')\n                current_series_number = original_series_number if original_series_number else (SUGGESTION_FLAG + mashed_volume_number if mashed_volume_number else '')\n                current_publication_year = final_original_year if final_original_year else (SUGGESTION_FLAG + mashed_publication_year if mashed_publication_year else '')\n\n                if is_problematic:\n                    st_logger.info(f\"Final merged data for '{title}': {{'Call Number': current_call_number, 'Series Info': current_series_name, 'Series Number': current_series_number, 'Copyright Year': current_publication_year}}\")\n\n                # Collect for Vertex AI batch processing if still unclassified\n                if not current_call_number or current_call_number == \"UNKNOWN\":\n                    unclassified_books_for_vertex_ai.append({\n                        'title': title,\n                        'author': author,\n                        'row_index': row_index, # Keep track of original row index\n                        'lc_meta': lc_meta # Keep original metadata for later merging\n                    })\n                \n                results[row_index] = {\n                    'Title': capitalize_title_mla(clean_title(title)),\n                    'Author': clean_author(author),\n                    'Holdings Barcode': original_holding_barcode,\n                    'Call Number': current_call_number,\n                    'Series Info': current_series_name,\n                    'Series Number': current_series_number,\n                    'Copyright Year': current_publication_year\n                }\n                progress_bar.progress((i + 1) / len(df))\n\n        # Second pass: Batch process unclassified books with Vertex AI\n        if unclassified_books_for_vertex_ai:\n            st.write(\"Attempting Vertex AI batch classification for remaining books...\")\n            BATCH_SIZE = 5\n            # Group unclassified books into batches\n            batches = [unclassified_books_for_vertex_ai[j:j + BATCH_SIZE] for j in range(0, len(unclassified_books_for_vertex_ai), BATCH_SIZE)]\n            \n            with ThreadPoolExecutor(max_workers=1) as executor: # Use single worker for batch calls to avoid hitting rate limits too fast\n                batch_futures = {executor.submit(get_vertex_ai_classification_batch, batch, vertex_ai_credentials): batch for batch in batches}\n                \n                for future in as_completed(batch_futures):\n                    processed_batch = batch_futures[future]\n                    batch_classifications = future.result()\n                    \n                    for book_data in processed_batch:\n                        title = book_data['title']\n                        author = book_data['author']\n                        row_index = book_data['row_index']\n                        lc_meta = book_data['lc_meta']\n                        \n                        st_logger.debug(f\"--- Processing Vertex AI results for row {row_index}, title: {title} ---\")\n                        st_logger.debug(f\"results[{row_index}] before update: {results[row_index]}\")\n                        st_logger.debug(f\"lc_meta before update: {lc_meta}\")\n\n                        vertex_ai_results = {}\n                        cleaned_title = re.sub(r'[^a-zA-Z0-9\\s]', '', title).lower()\n                        for item in batch_classifications:\n                            item_title = item.get('title', '')\n                            if isinstance(item_title, str):\n                                cleaned_item_title = re.sub(r'[^a-zA-Z0-9\\s]', '', item_title).lower()\n                                if (cleaned_item_title in cleaned_title or cleaned_title in cleaned_item_title) and author.lower() in item.get('author', '').lower():\n                                    vertex_ai_results = item\n                                    break\n                        st_logger.debug(f\"vertex_ai_results: {vertex_ai_results}\")\n\n                        # Replace \"Unknown\" with empty string\n                        for k, v in vertex_ai_results.items():\n                            if v == \"Unknown\":\n                                vertex_ai_results[k] = \"\"\n\n                        # Update the classification in lc_meta for this book\n                        if vertex_ai_results.get('classification'):\n                            lc_meta['classification'] = vertex_ai_results['classification']\n                            if 'google_genres' not in lc_meta or not isinstance(lc_meta['google_genres'], list):\n                                lc_meta['google_genres'] = []\n                            lc_meta['google_genres'].append(vertex_ai_results['classification'])\n                        \n                        if vertex_ai_results.get('series_title'):\n                            lc_meta['series_name'] = vertex_ai_results['series_title']\n                        if vertex_ai_results.get('volume_number'):\n                            lc_meta['volume_number'] = vertex_ai_results['volume_number']\n                        if vertex_ai_results.get('copyright_year'):\n                            lc_meta['publication_year'] = vertex_ai_results['copyright_year']\n\n                        st_logger.debug(f\"lc_meta after update: {lc_meta}\")\n\n                        # Re-clean call number with new Vertex AI classification\n                        final_call_number_after_vertex_ai = clean_call_number(lc_meta.get('classification', ''), lc_meta.get('genres', []), lc_meta.get('google_genres', []), title=title)\n                        \n                        # Update the results list with the new classification\n                        results[row_index]['Call Number'] = (SUGGESTION_FLAG + final_call_number_after_vertex_ai if final_call_number_after_vertex_ai else \"\")\n                        results[row_index]['Series Info'] = lc_meta.get('series_name', '')\n                        results[row_index]['Series Number'] = str(lc_meta.get('volume_number', ''))\n                        results[row_index]['Copyright Year'] = str(lc_meta.get('publication_year', ''))\n                        st_logger.debug(f\"results[{row_index}] after update: {results[row_index]}\")\n\n        # Final pass to populate Series Info and ensure all fields are non-blank\n        for i, row_data in enumerate(results):\n            # Re-combine series name and volume number for display after all processing\n            final_series_name = row_data['Series Info'] # This was populated with mashed_series_name\n            final_series_number = row_data['Series Number']\n\n            results[i]['Series Info'] = final_series_name\n            results[i]['Series Number'] = final_series_number\n\n        save_cache(loc_cache)\n        \n        st.write(\"Processing complete!\")\n        \n        # Create a DataFrame from results\n        results_df = pd.DataFrame(results)\n\n        st.subheader(\"Processed Data\")\n        # Display editable DataFrame\n        edited_df = st.data_editor(results_df, use_container_width=True, hide_index=True)\n\n        st.info(\"Values marked with üêí are suggestions from external APIs. The monkey emoji will not appear on printed labels, but the suggested values will be used.\")\n\n        if st.button(\"Apply Manual Classifications and Update Cache\"):\n            updated_count = 0\n            current_cache = load_cache()\n            for index, row in edited_df.iterrows():\n                original_row = results_df.loc[index] # Get original row to form key\n                title = original_row['Title'].strip()\n                author = original_row['Author'].strip()\n                manual_key = f\"{re.sub(r'[^a-zA-Z0-9\\s\\.:]', '', title)}|{re.sub(r'[^a-zA-Z0-9\\s,]', '', author)}\".lower()\n\n                # Update cache only if the cleaned call number has changed\n                if row['Call Number'] != original_row['Call Number']:\n                    # Remove monkey emoji before saving to cache\n                    current_cache[manual_key] = row['Call Number'].replace(SUGGESTION_FLAG, '')\n                    updated_count += 1\n            save_cache(current_cache)\n            st.success(f\"Updated {updated_count} manual classifications in cache!\")\n            st.rerun()\n\n        # PDF Generation Section\n        st.subheader(\"Generate PDF Labels\")\n        if st.button(\"Generate PDF\"):\n            pdf_output = generate_pdf_labels(edited_df)\n            st.download_button(\n                label=\"Download PDF Labels\",\n                data=pdf_output,\n                file_name=\"book_labels.pdf\",\n                mime=\"application/pdf\"\n            )\n\n    with st.expander(\"Debug Log\"):\n        st.download_button(\n            label=\"Download Full Debug Log\",\n            data=log_capture_string.getvalue(),\n            file_name=\"debug_log.txt\",\n            mime=\"text/plain\"\n        )\n\nif __name__ == \"__main__\":\n    main()",
    "requirements_txt": "streamlit\npandas\nrequests\nlxml\nbeautifulsoup4\nvertexai\npytz"
  },
  "session_context": {
    "timestamp": "2025-08-13T08:00:00Z",
    "platform": "Gemini CLI",
    "user_os": "android"
  }
}