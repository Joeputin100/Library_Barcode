{
    "project_name": "Cost-Optimized Natural Language to SQL Query Translator for MARC Records",
    "notes": "Implement a cost-optimized natural language to SQL query translator for a MARC records database using Google Cloud Storage (GCS), BigQuery, LlamaIndex, and Retrieval-Augmented Generation (RAG) with Vertex AI Gemini, focusing on minimizing costs through caching, serverless execution, and efficient data handling during development and production. Key Priorities: Cost Optimization, Accuracy for Novel Inputs, Scalability and Development Efficiency. Required Tools: Google Cloud Project (GCS, BigQuery, Vertex AI, Cloud Functions, Dataflow), Python libraries (pymarc, google-cloud-bigquery, google-cloud-aiplatform, llama-index, langchain-google-vertexai), Caching (Redis/local file-based), Development Environment (Colab/local with gcloud auth).",
    "phases": [
        {
            "phase_name": "Phase 1: Data Ingestion and Storage Setup",
            "description": "Ingest the .marc file into GCS and BigQuery with cost-efficient parsing and schema definition.",
            "tasks": [
                {
                    "task_id": "1.1",
                    "task_name": "MARC to JSON Conversion and GCS Upload",
                    "status": "DONE",
                    "model": "Flash",
                    "description": "Develop a Cloud Function (Python) triggered by GCS uploads to parse .marc using pymarc, convert to NDJSON (flattening key fields like title, author, barcode), and store in GCS. Cache parsed files locally during dev to avoid re-parsing.",
                    "dependencies": [],
                    "expected_output": "Cloud Function deployed; NDJSON files in GCS bucket."
                },
                {
                    "task_id": "1.2",
                    "task_name": "BigQuery Table Creation and Loading",
                    "status": "DONE",
                    "model": "Flash",
                    "description": "Create BigQuery dataset/table with autodetected schema, refined for MARC (e.g., JSON column for variable fields). Load NDJSON from GCS using bq load --autodetect. Enable table expiration or partitioning by ingest date to control storage costs.",
                    "dependencies": [
                        "1.1"
                    ],
                    "expected_output": "BigQuery table populated; schema documented."
                },
                {
                    "task_id": "1.3",
                    "task_name": "Schema Embedding for RAG",
                    "status": "DONE",
                    "model": "Flash",
                    "description": "Generate schema description (columns, MARC mappings, examples) and embed using Vertex AI textembedding-gecko. Store embeddings in a simple GCS file or Vector Search index. Cache embeddings to reuse across sessions.",
                    "dependencies": [
                        "1.2"
                    ],
                    "expected_output": "Embedded schema ready for retrieval."
                },
                {
                    "task_id": "1.4",
                    "task_name": "MARC to JSON Conversion",
                    "status": "DONE",
                    "description": "Create a script or process to convert raw MARC data into a structured JSON format.",
                    "model": "Pro",
                    "dependencies": []
                }
            ]
        },
        {
            "phase_name": "Phase 2: NL Query Processing with LlamaIndex and RAG",
            "description": "Build the text-to-SQL pipeline with caching to minimize Vertex AI calls and BigQuery queries.",
            "tasks": [
                {
                    "task_id": "2.1",
                    "task_name": "LlamaIndex Pipeline Setup",
                    "status": "DONE",
                    "model": "Pro",
                    "description": "Configure LlamaIndex with BigQuery as backend (via SQLAlchemy). Use VectorStoreIndex for schema embeddings. Implement SQLTableRetrieverQueryEngine to retrieve relevant schema snippets via RAG.",
                    "dependencies": [
                        "1.3"
                    ],
                    "expected_output": "LlamaIndex engine initialized in a script/module."
                },
                {
                    "task_id": "2.2",
                    "task_name": "Prompt Engineering and Gemini Integration",
                    "status": "DONE",
                    "model": "Pro",
                    "description": "Define dynamic prompts including retrieved schema, few-shot examples (cached in a file), and safeguards. Integrate Vertex AI Gemini (gemini-2.5-flash for cost efficiency). Cache prompt-response pairs using a dict or Redis to skip API calls for repeated queries.",
                    "dependencies": [
                        "2.1"
                    ],
                    "expected_output": "nl_to_sql function that generates, validates, and caches SQL."
                },
                {
                    "task_id": "2.3",
                    "task_name": "Query Execution and Caching",
                    "status": "DONE",
                    "model": "Pro",
                    "description": "Execute generated SQL on BigQuery, cache results (e.g., via BigQuery Storage API for efficiency). Implement self-correction: If SQL invalid, reprompt (but cache failures to avoid loops). Use materialized views in BigQuery for frequent queries to reduce compute costs.",
                    "dependencies": [
                        "2.2"
                    ],
                    "expected_output": "End-to-end query flow with caching; test with sample NL inputs."
                },
                {
                    "task_id": "2.4",
                    "task_name": "Accuracy and Cost Testing",
                    "status": "DONE",
                    "model": "Pro",
                    "description": "Benchmark on 50+ novel queries; measure hit rate of cache (>80% during dev). Monitor costs via Billing API; aim for < $0.01 per query post-caching.",
                    "dependencies": [
                        "2.3"
                    ],
                    "expected_output": "Test suite passing; cost report."
                }
            ]
        },
        {
            "phase_name": "Phase 3: Integration and Reintegration",
            "description": "Integrate into apps and enable MARC export for Atriuum overlay.",
            "tasks": [
                {
                    "task_id": "3.1",
                    "task_name": "App Integration",
                    "status": "DONE",
                    "model": "Flash",
                    "description": "Update streamlit_app.py and marc_query_tui.py to use the new pipeline. Add cache clear button for dev.",
                    "dependencies": [
                        "2.3"
                    ],
                    "expected_output": "Apps refactored and tested."
                },
                {
                    "task_id": "3.2",
                    "task_name": "MARC Export for Atriuum",
                    "status": "DONE",
                    "model": "Pro",
                    "description": "Query BigQuery for enriched data, reconstruct MARC using pymarc, export to .marc in GCS. Include matching fields (ISBN, Title+Author) for overlay. Cache exports for repeated use.",
                    "dependencies": [
                        "1.2"
                    ],
                    "expected_output": "Export function; documentation on Atriuum import steps."
                },
                {
                    "task_id": "3.3",
                    "task_name": "Full Workflow Testing",
                    "status": "DONE",
                    "model": "Flash",
                    "description": "End-to-end test: Ingest, query NL, enrich, export. Verify cost savings with caching enabled.",
                    "dependencies": [
                        "3.2"
                    ],
                    "expected_output": "Successful tests; optimized for minimal costs."
                },
                {
                    "task_id": "3.4",
                    "task_name": "Implement Loading Indicators in TUIs",
                    "status": "DONE",
                    "model": "Flash",
                    "description": "Use textual.widgets.LoadingIndicator to show a loading indicator when waiting for API responses in both project_viewer.py and marc_query_tui.py.",
                    "dependencies": [],
                    "expected_output": "Loading indicators displayed during API calls in TUIs."
                },
                {
                    "task_id": "3.5",
                    "task_name": "Maintain Input Focus in MARC Query TUI",
                    "status": "DONE",
                    "model": "Flash",
                    "description": "Ensure the input widget in marc_query_tui.py retains focus after user interactions or updates.",
                    "dependencies": [],
                    "expected_output": "Input widget in marc_query_tui.py remains focused."
                },
                {
                    "task_id": "3.6",
                    "task_name": "Add Visual Separators to TUIs",
                    "status": "DONE",
                    "model": "Flash",
                    "description": "Use textual.widgets.Rule to visually separate widgets in both project_viewer.py and marc_query_tui.py for improved readability.",
                    "dependencies": [],
                    "expected_output": "Visual separators (rules) added to TUIs."
                }
            ]
        },
        {
            "phase_name": "Phase 7: Streamlit and MARC Integration",
            "description": "Integrate MARC handling and querying directly into the Streamlit application.",
            "tasks": [
                {
                    "task_id": "7.1",
                    "task_name": "Restate MARC Query in Natural English",
                    "status": "DONE",
                    "model": "Pro",
                    "description": "When the MARC query TUI asks for confirmation, have it restate the query in natural English for better user understanding.",
                    "dependencies": [],
                    "expected_output": "MARC query TUI shows a natural language version of the query before execution."
                },
                {
                    "task_id": "7.2",
                    "task_name": "Integrate MARC Import and Query into Streamlit",
                    "status": "DONE",
                    "model": "Pro",
                    "description": "In the Streamlit app, provide options to import a MARC or CSV file, or use an existing MARC database, showing a list of available databases.",
                    "dependencies": [],
                    "expected_output": "Streamlit app has a new data import step with MARC and CSV options."
                },
                {
                    "task_id": "7.3",
                    "task_name": "Use Bibliographic Data for API Queries",
                    "status": "DONE",
                    "model": "Pro",
                    "description": "After importing, use data from the bibliographic records (ISBN, LCCN) to query external data sources, with a fallback to author and title if necessary.",
                    "dependencies": ["7.2"],
                    "expected_output": "Data enrichment process prioritizes bibliographic data for API queries."
                },
                {
                    "task_id": "7.4",
                    "task_name": "Confirm Changes Before Saving",
                    "status": "DONE",
                    "model": "Flash",
                    "description": "After the manual row editing step in the Streamlit app, ask the user for confirmation before saving the changes to the database.",
                    "dependencies": ["7.2"],
                    "expected_output": "A confirmation dialog appears before saving edits in the Streamlit app."
                },
                {
                    "task_id": "7.5",
                    "task_name": "Generate MARC Export File for Changed Rows",
                    "status": "DONE",
                    "model": "Pro",
                    "description": "For rows that have been changed (either by API enrichment or manual edits), generate a MARC import file that can be uploaded back to Atriuum.",
                    "dependencies": ["7.3", "7.4"],
                    "expected_output": "A .marc file is generated containing only the changed records."
                }
            ]
        },
        {
            "phase_name": "Phase 8: BigQuery Migration",
            "description": "Migrate MARC data processing and enrichment to BigQuery for scalability and performance.",
            "tasks": [
                {
                    "task_id": "8.1",
                    "task_name": "Migrate MARC Data Processing to BigQuery",
                    "status": "DONE",
                    "model": "Pro",
                    "priority": "High",
                    "description": "Refactor the data processing pipeline to perform data enrichment and cleaning operations directly within BigQuery using SQL queries and UDFs, leveraging the logic already developed in the local scripts.",
                    "dependencies": [],
                    "expected_output": "The Streamlit app will query BigQuery for processed data instead of performing local processing."
                }
            ]
        },
        {
            "phase_name": "Phase 9: Code Quality and Maintenance",
            "description": "Ensuring code quality and maintainability through automated checks and standards.",
            "tasks": [
                {
                    "task_id": "9.1",
                    "task_name": "Implement and Enforce Linter Checks",
                    "status": "DONE",
                    "model": "Flash",
                    "description": "Configure and apply black and flake8 to the entire codebase to ensure consistent code style and quality. Integrate these checks into a pre-commit hook to automatically enforce standards.",
                    "dependencies": [],
                    "expected_output": "A clean and consistently formatted codebase, with automated checks to maintain this standard."
                },
                {
                    "task_id": "9.2",
                    "task_name": "Fix Project Viewer Tree Auto-Collapse",
                    "status": "DONE",
                    "model": "Flash",
                    "description": "Removed automatic tree reloading in project_viewer.py to prevent auto-collapse and added a manual refresh button.",
                    "dependencies": [],
                    "expected_output": "Project viewer tree remains expanded as left by user."
                }
            ]
        },
        {
            "phase_name": "Phase 10: Deployment and Configuration",
            "description": "Tasks related to deploying the application and configuring the environment.",
            "tasks": [
                {
                    "task_id": "10.1",
                    "task_name": "Configure Application Default Credentials (ADC)",
                    "status": "DONE",
                    "model": "Flash",
                    "description": "Create a step-by-step guide or a script to help the user configure Application Default Credentials (ADC) in their Termux environment, so the application can authenticate with Google Cloud services.",
                    "dependencies": [],
                    "expected_output": "A clear guide or script that allows the user to successfully authenticate the application."
                }
            ]
        }
    ]
}